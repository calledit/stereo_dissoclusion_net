# stereo_disocclusion_net
Neural network for infilling disoccluded regions produced by stereo warping and re-rendering.

## Overview
When re-rendering an image from a nearby stereo viewpoint (e.g., shifting the camera horizontally),
some regions become *disoccluded*—areas visible in the target view that were hidden in the source view.
These regions appear as holes after depth-based warping and must be filled in to reconstruct a
coherent image.

`stereo_disocclusion_net` is intended as a lightweight, fast neural infill model designed specifically for these
stereo disocclusion holes. The model uses a combination of:
- the warped image with missing regions,
- the hole mask,
- **depth** (the (warped) orginnal depth),
- **normals inside the holes** (derived from the (warped) orginal depth. )

…to reconstruct a plausible infilled target image.

This project can generate esentially **infinite synthetic training data** from arbitrary internet images using
automatic depth estimation and mesh-based view synthesis.

## Key Features (Intended)
- **Stereo-disocclusion–aware infill**  
  The network is optimized for the unique geometry of stereo reprojection holes, not generic 2D inpainting.

- **Normals for hole guidance**  
  Inside disocclusions, surface normals derived from the warped depth indicate which direction
  the missing texture should continue from.

- **Depth for global structure**  
  The warped depth anchors the scene’s global geometry and helps the network keep
  textures and structures consistent.

- **Lightweight architecture**  
  Powered by a Gated-UNet (gated convolutions + UNet backbone) for fast training and real-time-friendly inference.

- **Fully synthetic training data pipeline**  
  Uses any RGB image + monocular depth model → builds meshes → renders left/right → produces
  realistic stereo warping artifacts including disocclusions.

## Input Representation
Each training and inference sample uses an 8-channel input tensor:

| Channels | Description |
|---------|-------------|
| 3       | Warped RGB image (`warped_image`) with holes filled with bad data |
| 1       | Hole mask (`hole_mask`) (1 = hole, 0 = valid pixel) |
| 3       | Normals **inside holes only** (`normals_masked`), zeroed elsewhere |
| 1       | Depth (`depth_masked`), zeroed inside holes |

The representation outover warped_image and hole_mask gives the network:
- *local geometric cues* (normals inside the hole), and  
- *global geometric context* (depth).

## Output
The network predicts:
- **infilled RGB image** for the input view.

*(Optionally, the network may also be trained to infill depth instead of RGB.)*

---

## Data Generation Pipeline
Training data is generated automatically from any collection of images:

1. **Start from an online RGB image**  
   Treat this as if it were the left image of a stereo pair: `org_left_image`.

2. **Predict monocular metric depth**  
    Use a metric depth model (e.g., **da3** or **unik3d**) to produce `org_left_depth`.

3. **Build a mesh** from RGB + depth
   → `org_left_mesh`.

4. **Render a synthetic right-eye view**  
   Shift the camera horizontally (e.g., 63 mm) to produce:  
   - `generated_right_image`  
   - `generated_right_depth`

5. **Build a right-view mesh** from generated RGB + depth
   Tag faces with extreme normals (these faces cause disocclusions when reprojected).  
   → `generated_right_mesh`.

6. **Render this mesh to the left-eye view**  
   By shifting camera horizontally back (63 mm) producing:  
   - `image_generated_left` (with stereo disocclusion issues)
   - `hole_mask` (generated from the taged faces in `image_generated_left`)
   - `depth_generated_left`  
   - `normals_generated_left` (generated from `depth_generated_left`)
   - `normals_left_masked` (normals masked with `hole_mask`)

7. **Use original left image as ground truth** 
   `org_left_image` is the supervised target (for RGB infill), or `org_left_depth` if training depth infill.
   those two are bound to the generated `hole_mask`, `normals_left_masked` and `depth_generated_left`.

The pipeline creates unlimited, highly realistic training samples tuned specifically for stereo warping and disocclusion artifacts.  
Left/right can be swapped with 50% probability for better generalization.


## Goals
- Real-time or near-real-time inference for stereo reconstruction.
- Robust handling of large disocclusions generated by depth-based warping.
- Optionally extend to depth completion using the same architecture.

## Status
Early development.


---
## Install
```bash
git clone https://github.com/calledit/stereo_dissoclusion_net
cd stereo_dissoclusion_net
git clone https://github.com/calledit/metric_depth_video_toolbox

To get traning data:
./download_coco_traning_dataset.sh
edit absolute paths in:
generate_traning_data_from_dataset.py
then:
python generate_traning_data_from_dataset.py

```

## TODO
- [x] Traning data generation scripts  
- [ ] Write network 
- [ ] Upload pretrained weights  
- [ ] Visual examples of disocclusion infill  
